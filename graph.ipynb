{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "# from matplotlib import style\n",
    "import random\n",
    "import pickle\n",
    "# style.use(\"dark_background\")\n",
    "seed_val = 994\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>also I was the point person on my company’s tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:16,059</td>\n",
       "      <td>00:16:21,731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>You must’ve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:21,940</td>\n",
       "      <td>00:16:23,442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:23,442</td>\n",
       "      <td>00:16:26,389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So let’s talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:26,820</td>\n",
       "      <td>00:16:29,572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:34,452</td>\n",
       "      <td>00:16:40,917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance          Speaker  \\\n",
       "0       1  also I was the point person on my company’s tr...         Chandler   \n",
       "1       2                   You must’ve had your hands full.  The Interviewer   \n",
       "2       3                            That I did. That I did.         Chandler   \n",
       "3       4      So let’s talk a little bit about your duties.  The Interviewer   \n",
       "4       5                             My duties?  All right.         Chandler   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0   neutral   neutral            0             0       8       21   \n",
       "1   neutral   neutral            0             1       8       21   \n",
       "2   neutral   neutral            0             2       8       21   \n",
       "3   neutral   neutral            0             3       8       21   \n",
       "4  surprise  positive            0             4       8       21   \n",
       "\n",
       "      StartTime       EndTime  \n",
       "0  00:16:16,059  00:16:21,731  \n",
       "1  00:16:21,940  00:16:23,442  \n",
       "2  00:16:23,442  00:16:26,389  \n",
       "3  00:16:26,820  00:16:29,572  \n",
       "4  00:16:34,452  00:16:40,917  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %history\n",
    "train_df=pd.read_csv(\"./MELD/data/MELD/train_sent_emo.csv\")\n",
    "val_df=pd.read_csv(\"./MELD/data/MELD/dev_sent_emo.csv\")\n",
    "test_df=pd.read_csv(\"./MELD/data/MELD/test_sent_emo.csv\")\n",
    "\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "# file = open(\"IEMOCAP_features_bert.pkl\",'rb')\n",
    "# object_file = pickle.load(file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4710\n",
      "4    1743\n",
      "1    1205\n",
      "6    1109\n",
      "3     683\n",
      "5     271\n",
      "2     268\n",
      "Name: Emotion, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAATGUlEQVR4nO3dfYxc1XnH8e8TDBjhFEOgK2RbXdNYjUjdELICokTRAop5q2oqkcgNCk7kylJLpESiakyj1HkByalCaJESUrdYMVESQ0kQFqSlrmEU8QdvDu9Q4g2YYotgBRsnSxJak6d/zFkycXa94/HszI7P9yON5t5zz73z3Dvr39w9c/c6MhNJUh3e0u8CJEm9Y+hLUkUMfUmqiKEvSRUx9CWpInP6XcDBnHzyyTk8PNzRuq+99hrHH398dwvqgUGtG6y9Hwa1brD2mbRt27afZuYpky2b1aE/PDzMww8/3NG6jUaD0dHR7hbUA4NaN1h7Pwxq3WDtMykiXphqmcM7klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUkVn9F7mHa3jNXX153R3rLunL60rSdDzTl6SKtBX6EbEjIp6IiEcj4uHSdlJEbImI7eX5xNIeEXFDRIxFxOMRcWbLdlaW/tsjYuXM7JIkaSqHcqZ/bmaekZkjZX4NsDUzlwBbyzzARcCS8lgN3AjNDwlgLXA2cBawduKDQpLUG4czvLMc2FimNwKXtrTfnE33A/Mj4lTgAmBLZu7JzL3AFuDCw3h9SdIhisycvlPE88BeIIF/zsz1EfFqZs4vywPYm5nzI+JOYF1m3leWbQU+DYwCczPzmtL+WeCXmfnlA15rNc3fEBgaGnrPpk2bOtqx8fFxnt/3RkfrHq6lC07oeN3x8XHmzZvXxWp6x9p7b1DrBmufSeeee+62llGZ39Lu1Tvvz8xdEfH7wJaI+O/WhZmZETH9p0cbMnM9sB5gZGQkO71ndaPR4Lr7XutGSYdsx+WjHa872+/TfTDW3nuDWjdYe7+0NbyTmbvK827gdppj8i+XYRvK8+7SfRewqGX1haVtqnZJUo9MG/oRcXxEvHViGlgGPAlsBiauwFkJ3FGmNwNXlKt4zgH2ZeZLwN3Asog4sXyBu6y0SZJ6pJ3hnSHg9uawPXOAb2fmf0TEQ8CtEbEKeAH4cOn/feBiYAz4BfBxgMzcExFfBB4q/b6QmXu6tieSpGlNG/qZ+RzwrknaXwHOn6Q9gSun2NYGYMOhlylJ6gb/IleSKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIq0HfoRcVREPBIRd5b5xRHxQESMRcQtEXFMaT+2zI+V5cMt27i6tD8bERd0fW8kSQd1KGf6nwSeaZn/EnB9Zr4d2AusKu2rgL2l/frSj4g4HVgBvBO4EPhaRBx1eOVLkg5FW6EfEQuBS4B/LfMBnAfcVrpsBC4t08vLPGX5+aX/cmBTZr6emc8DY8BZXdgHSVKb2j3T/0fgb4Ffl/m3Aa9m5v4yvxNYUKYXAC8ClOX7Sv832ydZR5LUA3Om6xARfwrszsxtETE60wVFxGpgNcDQ0BCNRqOj7YyPj3PV0je6WFn7Oq0ZmnUfzvr9ZO29N6h1g7X3y7ShD7wP+LOIuBiYC/we8E/A/IiYU87mFwK7Sv9dwCJgZ0TMAU4AXmlpn9C6zpsycz2wHmBkZCRHR0c72K1m8F5332sdrXu4dlw+2vG6jUaDTve536y99wa1brD2fpl2eCczr87MhZk5TPOL2Hsy83LgXuCy0m0lcEeZ3lzmKcvvycws7SvK1T2LgSXAg13bE0nStNo505/Kp4FNEXEN8AhwU2m/CfhmRIwBe2h+UJCZT0XErcDTwH7gyszsz/iLJFXqkEI/MxtAo0w/xyRX32Tmr4APTbH+tcC1h1qkJKk7/ItcSaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkWmDf2ImBsRD0bEYxHxVER8vrQvjogHImIsIm6JiGNK+7FlfqwsH27Z1tWl/dmIuGDG9kqSNKl2zvRfB87LzHcBZwAXRsQ5wJeA6zPz7cBeYFXpvwrYW9qvL/2IiNOBFcA7gQuBr0XEUV3cF0nSNKYN/WwaL7NHl0cC5wG3lfaNwKVlenmZpyw/PyKitG/KzNcz83lgDDirGzshSWrPnHY6lTPybcDbga8CPwZezcz9pctOYEGZXgC8CJCZ+yNiH/C20n5/y2Zb12l9rdXAaoChoSEajcah7VExPj7OVUvf6Gjdw9VpzdCs+3DW7ydr771BrRusvV/aCv3MfAM4IyLmA7cD75ipgjJzPbAeYGRkJEdHRzvaTqPR4Lr7XutiZe3bcflox+s2Gg063ed+s/beG9S6wdr7pa3Qn5CZr0bEvcB7gfkRMaec7S8EdpVuu4BFwM6ImAOcALzS0j6hdZ0jyvCauzpe96ql+/lYh+vvWHdJx68rqQ7tXL1zSjnDJyKOAz4IPAPcC1xWuq0E7ijTm8s8Zfk9mZmlfUW5umcxsAR4sEv7IUlqQztn+qcCG8u4/luAWzPzzoh4GtgUEdcAjwA3lf43Ad+MiDFgD80rdsjMpyLiVuBpYD9wZRk2kiT1yLShn5mPA++epP05Jrn6JjN/BXxoim1dC1x76GVKkrrBv8iVpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jaki04Z+RCyKiHsj4umIeCoiPlnaT4qILRGxvTyfWNojIm6IiLGIeDwizmzZ1srSf3tErJy53ZIkTaadM/39wFWZeTpwDnBlRJwOrAG2ZuYSYGuZB7gIWFIeq4EbofkhAawFzgbOAtZOfFBIknpj2tDPzJcy84dl+ufAM8ACYDmwsXTbCFxappcDN2fT/cD8iDgVuADYkpl7MnMvsAW4sJs7I0k6uMjM9jtHDAM/AP4Y+J/MnF/aA9ibmfMj4k5gXWbeV5ZtBT4NjAJzM/Oa0v5Z4JeZ+eUDXmM1zd8QGBoaes+mTZs62rHx8XGe3/dGR+v209Bx8PIvO1t36YITulvMIRofH2fevHl9raFTg1r7oNYN1j6Tzj333G2ZOTLZsjntbiQi5gHfBT6VmT9r5nxTZmZEtP/pcRCZuR5YDzAyMpKjo6MdbafRaHDdfa91o6Seumrpfq57ou235bfsuHy0u8UcokajQafvV78Nau2DWjdYe7+0dfVORBxNM/C/lZnfK80vl2EbyvPu0r4LWNSy+sLSNlW7JKlH2rl6J4CbgGcy8ystizYDE1fgrATuaGm/olzFcw6wLzNfAu4GlkXEieUL3GWlTZLUI+2MI7wP+CjwREQ8Wtr+DlgH3BoRq4AXgA+XZd8HLgbGgF8AHwfIzD0R8UXgodLvC5m5pxs7IUlqz7ShX76QjSkWnz9J/wSunGJbG4ANh1KgJKl7/ItcSaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkioyp98FqHuG19zVt9fese6Svr22pPZ5pi9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkWmDf2I2BARuyPiyZa2kyJiS0RsL88nlvaIiBsiYiwiHo+IM1vWWVn6b4+IlTOzO5Kkg2nnTP8bwIUHtK0BtmbmEmBrmQe4CFhSHquBG6H5IQGsBc4GzgLWTnxQSJJ6Z9rQz8wfAHsOaF4ObCzTG4FLW9pvzqb7gfkRcSpwAbAlM/dk5l5gC7/7QSJJmmGd3oZhKDNfKtM/AYbK9ALgxZZ+O0vbVO2/IyJW0/wtgaGhIRqNRkcFjo+P8413jXW07nSe+PXiGdkuwNBxcNXS/TO2/ZnSaDQYHx/v+P3qt0GtfVDrBmvvl8O+905mZkRkN4op21sPrAcYGRnJ0dHRjrbTaDQYfXhtt8r6LR/71bdnZLvQDPzrnhi8WyLtuHy0ecw7fL/6bVBrH9S6wdr7pdOrd14uwzaU592lfRewqKXfwtI2VbskqYc6Df3NwMQVOCuBO1raryhX8ZwD7CvDQHcDyyLixPIF7rLSJknqoWnHESLiO8AocHJE7KR5Fc464NaIWAW8AHy4dP8+cDEwBvwC+DhAZu6JiC8CD5V+X8jMA78cliTNsGlDPzP/YopF50/SN4Erp9jOBmDDIVUnSeoq/yJXkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKDN5NXmaBHXM/MiPbHZ7Be/pIEnimL0lVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRbxOX10xvOYurlq6n4+tuaunr7tj3SU9fT1p0HmmL0kV8Ux/Ftkx9yM03vJ5dsxd29Xt+pe+kiZ4pi9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkW8Tr8C/k9fkiZ4pi9JFTH0Jakihr4kVcQxfXXswO8KunXfIL8rkGaOZ/qSVBHP9DXQhrt0//5O/i8A7+WvQdTzM/2IuDAino2IsYhY0+vXl6Sa9fRMPyKOAr4KfBDYCTwUEZsz8+le1qHZzb8rkGZOr4d3zgLGMvM5gIjYBCwHDH3NuIN9mHT0JfTn+vNB4rCSDkdkZu9eLOIy4MLM/Msy/1Hg7Mz8REuf1cDqMvtHwLMdvtzJwE8Po9x+GdS6wdr7YVDrBmufSX+QmadMtmDWfZGbmeuB9Ye7nYh4ODNHulBSTw1q3WDt/TCodYO190uvv8jdBSxqmV9Y2iRJPdDr0H8IWBIRiyPiGGAFsLnHNUhStXo6vJOZ+yPiE8DdwFHAhsx8aoZe7rCHiPpkUOsGa++HQa0brL0vevpFriSpv7wNgyRVxNCXpIoccaE/aLd5iIgdEfFERDwaEQ+XtpMiYktEbC/PJ/a7ToCI2BARuyPiyZa2SWuNphvK+/B4RJw5y+r+XETsKsf90Yi4uGXZ1aXuZyPigv5U/WYtiyLi3oh4OiKeiohPlvZZfdwPUvesP+4RMTciHoyIx0rtny/tiyPigVLjLeViFCLi2DI/VpYP96v2tmTmEfOg+eXwj4HTgGOAx4DT+13XNDXvAE4+oO0fgDVleg3wpX7XWWr5AHAm8OR0tQIXA/8OBHAO8MAsq/tzwN9M0vf08nNzLLC4/Dwd1cfaTwXOLNNvBX5UapzVx/0gdc/6416O3bwyfTTwQDmWtwIrSvvXgb8q038NfL1MrwBu6dfPSzuPI+1M/83bPGTm/wITt3kYNMuBjWV6I3Bp/0r5jcz8AbDngOapal0O3JxN9wPzI+LUnhR6gCnqnspyYFNmvp6ZzwNjNH+u+iIzX8rMH5bpnwPPAAuY5cf9IHVPZdYc93Lsxsvs0eWRwHnAbaX9wGM+8V7cBpwfEdGbag/dkRb6C4AXW+Z3cvAftNkggf+MiG3lFhQAQ5n5Upn+CTDUn9LaMlWtg/BefKIMgWxoGUKbtXWXYYN30zzzHJjjfkDdMADHPSKOiohHgd3AFpq/ebyamftLl9b63qy9LN8HvK2nBR+CIy30B9H7M/NM4CLgyoj4QOvCbP7OOBDX1Q5SrcCNwB8CZwAvAdf1tZppRMQ84LvApzLzZ63LZvNxn6TugTjumflGZp5B864BZwHv6G9F3XOkhf7A3eYhM3eV593A7TR/wF6e+JW8PO/uX4XTmqrWWf1eZObL5R/2r4F/4TdDCbOu7og4mmZwfiszv1eaZ/1xn6zuQTruAJn5KnAv8F6aQ2UTf9DaWt+btZflJwCv9LbS9h1poT9Qt3mIiOMj4q0T08Ay4EmaNa8s3VYCd/SnwrZMVetm4IpyNck5wL6W4Yi+O2Cc+89pHndo1r2iXJGxGFgCPNjr+iaUseGbgGcy8ysti2b1cZ+q7kE47hFxSkTML9PH0fz/P56hGf6XlW4HHvOJ9+Iy4J7y29fs1O9vkrv9oHn1wo9ojsF9pt/1TFPraTSvWHgMeGqiXprjgVuB7cB/ASf1u9ZS13do/kr+fzTHNFdNVSvNKyC+Wt6HJ4CRWVb3N0tdj9P8R3tqS//PlLqfBS7q8zF/P82hm8eBR8vj4tl+3A9S96w/7sCfAI+UGp8E/r60n0bzg2gM+Dfg2NI+t8yPleWn9fNnZrqHt2GQpIocacM7kqSDMPQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRf4ft7Z7Kpi4JvYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "mapping ={'neutral':0, 'surprise':1, 'fear':2, 'sadness':3, 'joy':4, 'disgust':5, 'anger':6}\n",
    "\n",
    "# mapping2={\"Chandler\":0,\"Phoebe\":1,\"Monica\":2,\"Ross\":3,\"Joey\":4,\"Rachel\":5}\n",
    "\n",
    "train_df=train_df.replace({\"Emotion\":mapping})# mapping labels to integer\n",
    "val_df=val_df.replace({\"Emotion\":mapping})\n",
    "test_df=test_df.replace({\"Emotion\":mapping})\n",
    "\n",
    "# train_df=train_df.replace({\"Speaker\":mapping2})# mapping labels to integer\n",
    "# val_df=val_df.replace({\"Speaker\":mapping2})\n",
    "# test_df=test_df.replace({\"Speaker\":mapping2})\n",
    "\n",
    "\n",
    "train_text=train_df[\"Utterance\"].tolist()\n",
    "val_text=val_df[\"Utterance\"].tolist()\n",
    "test_text=test_df[\"Utterance\"].tolist()\n",
    "\n",
    "\n",
    "train_labels=train_df[\"Emotion\"]\n",
    "val_labels=val_df[\"Emotion\"]\n",
    "test_labels=test_df[\"Emotion\"]\n",
    "\n",
    "train_dia_id=train_df[\"Dialogue_ID\"]\n",
    "val_dia_id=val_df[\"Dialogue_ID\"]\n",
    "test_dia_id=test_df[\"Dialogue_ID\"]\n",
    "\n",
    "train_speaker=train_df[\"Speaker\"]\n",
    "val_speaker=val_df[\"Speaker\"]\n",
    "test_speaker=test_df[\"Speaker\"]\n",
    "\n",
    "# print(train_text,train_labels)\n",
    "\n",
    "len_train=[len(j) for j in train_text]\n",
    "len_test=[len(i) for i in test_text]\n",
    "\n",
    "pd.Series(len_train).hist()\n",
    "pd.Series(len_test).hist()\n",
    "\n",
    "print(train_df.Emotion.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see.\n",
      "Joey                1510\n",
      "Ross                1458\n",
      "Rachel              1435\n",
      "Phoebe              1321\n",
      "Monica              1299\n",
      "                    ... \n",
      "Phoebe/Waitress        1\n",
      "Vince                  1\n",
      "Gary Collins           1\n",
      "Hold Voice             1\n",
      "Front Desk Clerk       1\n",
      "Name: Speaker, Length: 260, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_text[6])\n",
    "print(train_df[\"Speaker\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "model = RobertaModel.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train=tokenizer(train_text,\n",
    "                                         max_length=80,\n",
    "                                         padding=True,\n",
    "                                         truncation=True)\n",
    "\n",
    "tokens_val=tokenizer(val_text,\n",
    "                                         max_length=80,\n",
    "                                         padding=True,\n",
    "                                         truncation=True)\n",
    "\n",
    "tokens_test=tokenizer(test_text,\n",
    "                                         max_length=80,\n",
    "                                         padding=True,\n",
    "                                         truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq=torch.tensor(tokens_train[\"input_ids\"])\n",
    "train_mask=torch.tensor(tokens_train[\"attention_mask\"])\n",
    "train_label=torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq=torch.tensor(tokens_val[\"input_ids\"])\n",
    "val_mask=torch.tensor(tokens_val[\"attention_mask\"])\n",
    "val_label=torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq=torch.tensor(tokens_test[\"input_ids\"])\n",
    "test_mask=torch.tensor(tokens_test[\"attention_mask\"])\n",
    "test_label=torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset,RandomSampler, SequentialSampler\n",
    "batch_size=32\n",
    "\n",
    "train_data=TensorDataset(train_seq,train_mask,train_label)\n",
    "train_sampler=SequentialSampler(train_data)\n",
    "train_dataloader=DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "val_data=TensorDataset(val_seq,val_mask,val_label)\n",
    "val_sampler=SequentialSampler(val_data)\n",
    "val_dataloader=DataLoader(val_data,sampler=val_sampler,batch_size=batch_size)\n",
    "\n",
    "test_data=TensorDataset(test_seq,test_mask,test_label)\n",
    "print(len(test_data[0][0]))\n",
    "test_sampler=SequentialSampler(test_data)\n",
    "test_dataloader=DataLoader(test_data,sampler=test_sampler,batch_size=batch_size)\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "print(type(train_speaker))\n",
    "# modules=[bert.encoder.layer[:1]]\n",
    "# for module in modules:\n",
    "#     for param in module.parameters():\n",
    "#         param.requires_grad=False\n",
    "\n",
    "# for name, param in list(bert.named_parameters())[:-79]: \n",
    "#     print('I will be frozen: {}'.format(name)) \n",
    "#     param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=BERT_model(bert)\n",
    "# model=AutoModel.from_pretrained(\"roberta-large\")\n",
    "\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model.eval()\n",
    "train_embed=[]\n",
    "val_embed=[]\n",
    "test_embed=[]\n",
    "with torch.no_grad():\n",
    "    for step,batch in tqdm_notebook(enumerate(test_dataloader),total=len(test_dataloader)):        \n",
    "            \n",
    "        batch=[r.to(device) for r in batch]\n",
    "        \n",
    "        sent_id,mask,labels=batch\n",
    "                \n",
    "        preds=model(sent_id,mask)[1]    # size  16*1024 batxh_size*encoding\n",
    "\n",
    "\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        test_embed.append(preds)\n",
    "        \n",
    "    for step,batch in tqdm_notebook(enumerate(val_dataloader),total=len(val_dataloader)):        \n",
    "            \n",
    "        batch=[r.to(device) for r in batch]\n",
    "        \n",
    "        sent_id,mask,labels=batch\n",
    "                \n",
    "        preds=model(sent_id,mask)[1]    # size  16*1024 batxh_size*encoding\n",
    "\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        val_embed.append(preds)\n",
    "        \n",
    "    for step,batch in tqdm_notebook(enumerate(train_dataloader),total=len(train_dataloader)):        \n",
    "            \n",
    "        batch=[r.to(device) for r in batch]\n",
    "        \n",
    "        sent_id,mask,labels=batch\n",
    "                \n",
    "        preds=model(sent_id,mask)[1]    # size  16*1024 batxh_size*encoding\n",
    "\n",
    "\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        train_embed.append(preds)\n",
    "        \n",
    "    train_embed=np.concatenate(train_embed,axis=0)\n",
    "    val_embed=np.concatenate(val_embed,axis=0)\n",
    "    test_embed= np.concatenate(test_embed,axis=0)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_preds= np.concatenate(total_preds,axis=0)\n",
    "print(len(train_embed))\n",
    "print(len(val_embed))\n",
    "print(len(test_embed))\n",
    "train_embed=torch.tensor(train_embed)  #changing to tensor\n",
    "val_embed=torch.tensor(val_embed)\n",
    "test_embed=torch.tensor(test_embed)\n",
    "print(train_embed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "import graph4nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self,in_f,o_f,num_label):\n",
    "        super().__init__()\n",
    "        self.conv1=GraphConv(in_f,o_f)\n",
    "        self.conv2=GraphConv(in_f,o_f)\n",
    "        self.conv3=GraphConv(in_f,o_f)\n",
    "        self.conv4=GraphConv(in_f,o_f)\n",
    "        self.conv5=GraphConv(in_f,o_f)\n",
    "        self.fc1=nn.Linear(2*o_f,num_label)\n",
    "        \n",
    "    def forward(self,g,in_f):\n",
    "        h=self.conv1(g,in_f)\n",
    "        h=F.relu(h)\n",
    "        h=self.conv2(g,h)\n",
    "        h=F.relu(h)\n",
    "        h=self.conv3(g,h)\n",
    "        h=F.relu(h)\n",
    "        h=self.conv4(g,h)\n",
    "        h=F.relu(h)\n",
    "        h=self.conv5(g,h)\n",
    "        h=F.relu(h)\n",
    "        h=self.fc1(torch.cat([in_f,h] ,dim=-1))[-1]\n",
    "#         h=F.softmax(h,dim=1)\n",
    "        \n",
    "        return h\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_speaker=train_speaker[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs=[]\n",
    "# u,v=torch.tensor([0,1,2]),torch.tensor([1,2,3])\n",
    "# g=dgl.graph((u,v))\n",
    "# g.ndata[\"emb\"]=train_embed[:g.num_nodes()]\n",
    "# print(g.device)\n",
    "for i,e in enumerate(train_embed):\n",
    "    if i>= (5):\n",
    "#         speakers=train_speaker[i-5:i+1]\n",
    "#         print(speakers)\n",
    "#         speakers=speakers.unique()\n",
    "#         print(speakers)\n",
    "#         main_speakers=[e for e in speakers if isinstance(e,int)]  \n",
    "#         for e in main_speakers:\n",
    "#             print(e)\n",
    "        u,v=torch.tensor([0,1,2,3,4]),torch.tensor([1,2,3,4,5])\n",
    "#         print(i)\n",
    "        g=dgl.graph((u,v))\n",
    "        g.ndata[\"emb\"]=train_embed[i-5:i+1]\n",
    "        \n",
    "        graphs.append(g)\n",
    "    \n",
    "    if i==4:\n",
    "        u,v=torch.tensor([0,1,2,3]),torch.tensor([1,2,3,4])\n",
    "        g=dgl.graph((u,v))\n",
    "        g.ndata[\"emb\"]=train_embed[0:5]\n",
    "        graphs.append(g)\n",
    "    \n",
    "    if i==3:\n",
    "        u,v=torch.tensor([0,1,2]),torch.tensor([1,2,3])\n",
    "        g=dgl.graph((u,v))\n",
    "        g.ndata[\"emb\"]=train_embed[0:4]\n",
    "        graphs.append(g)\n",
    "        \n",
    "    if i==2:\n",
    "        u,v=torch.tensor([0,1]),torch.tensor([1,2])\n",
    "        g=dgl.graph((u,v))\n",
    "        g.ndata[\"emb\"]=train_embed[0:3]\n",
    "        graphs.append(g)\n",
    "    if i==1:\n",
    "        u,v=torch.tensor([0]),torch.tensor([1])\n",
    "        g=dgl.graph((u,v))\n",
    "        g.ndata[\"emb\"]=train_embed[0:2]\n",
    "        graphs.append(g)\n",
    "#     if i==0:\n",
    "#         u,v=torch.tensor([0]),torch.tensor([0])\n",
    "#         g=dgl.graph((u,v),num_nodes=1)\n",
    "#         g.ndata[\"emb\"]=torch.tensor([train_embed[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(graphs))\n",
    "print(graphs[1].ndata[\"emb\"],graphs[2].ndata[\"emb\"])\n",
    "# torch.manual_seed(1)\n",
    "# a=torch.rand(2)\n",
    "# torch.manual_seed(1)\n",
    "# b=torch.rand(2)\n",
    "# f=[train_embed[0]]\n",
    "# f=[*range(0,2,1)]\n",
    "# print(len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GCN(1024,1024,7)\n",
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=train_df[\"Emotion\"]\n",
    "print(train_label.shape)\n",
    "# train_label=train_label[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_tot=0\n",
    "for epoch in range(50):\n",
    "    for i,graph in tqdm_notebook(enumerate(graphs),total=len(graphs)):\n",
    "        model.train()\n",
    "        node_features=graph.ndata[\"emb\"]\n",
    "        graph=dgl.add_self_loop(graph)\n",
    "        logits = model(graph, node_features)\n",
    "        logits=torch.unsqueeze(logits,0)\n",
    "        label=torch.unsqueeze(train_label[i],0)\n",
    "        loss=loss_fn(logits,label)\n",
    "        loss_tot+=loss.item()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print(\"epoch\",epoch)\n",
    "    print(loss_tot)\n",
    "    loss_tot=0\n",
    "    \n",
    "# graph = dgl.add_self_loop(graphs[1])\n",
    "# loss = F.cross_entropy(logits, train_label[i])\n",
    "# logits = model(graph, graph.ndata[\"emb\"])\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20.6336, 17.8999, 14.9298, 17.6984, 18.9020, 20.8048, 17.9433]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "# logits = torch.unsqueeze(logits,0)\n",
    "# print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2610\n",
      "2610\n"
     ]
    }
   ],
   "source": [
    "# x=torch.unsqueeze(train_label[1],0)\n",
    "# print(x.shape)\n",
    "# print(loss_fn(logits, x))\n",
    "# print(train_label[9].shape)\n",
    "print(len(test_embed))\n",
    "# test_embed=test_embed[1:]\n",
    "# print(train_label[9].shape)\n",
    "print(len(test_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_test=[]\n",
    "# u,v=torch.tensor([0,1,2]),torch.tensor([1,2,3])\n",
    "# g=dgl.graph((u,v))\n",
    "# g.ndata[\"emb\"]=train_embed[:g.num_nodes()]\n",
    "# print(g.device)\n",
    "for i,e in enumerate(test_embed):\n",
    "    if i>= (5):\n",
    "        u,v=torch.tensor([0,1,2,3,4]),torch.tensor([1,2,3,4,5])\n",
    "#         print(i)\n",
    "        g=dgl.graph((u,v))\n",
    "        g.ndata[\"emb\"]=test_embed[i-5:i+1]\n",
    "#         speakers=train_speaker[i-3:i+1]\n",
    "# #         print(speakers)\n",
    "#         speakers=speakers.unique()\n",
    "#         main_speakers=[e for e in speakers if isinstance(e,int)]\n",
    "#         for e in main_speakers\n",
    "#         print()\n",
    "        graphs_test.append(g)\n",
    "    \n",
    "    if i==4:\n",
    "        u,v=torch.tensor([0,1,2,3]),torch.tensor([1,2,3,4])\n",
    "        g=dgl.graph((u,v))\n",
    "        g.ndata[\"emb\"]=test_embed[0:5]\n",
    "        graphs_test.append(g)\n",
    "        \n",
    "    if i==3:\n",
    "        u,v=torch.tensor([0,1,2]),torch.tensor([1,2,3])\n",
    "        g=dgl.graph((u,v))\n",
    "        g.ndata[\"emb\"]=test_embed[0:4]\n",
    "        graphs_test.append(g)\n",
    "    if i==2:\n",
    "        u,v=torch.tensor([0,1]),torch.tensor([1,2])\n",
    "        g=dgl.graph((u,v))\n",
    "        g.ndata[\"emb\"]=test_embed[0:3]\n",
    "        graphs_test.append(g)\n",
    "    if i==1:\n",
    "        u,v=torch.tensor([0]),torch.tensor([1])\n",
    "        g=dgl.graph((u,v))\n",
    "        g.ndata[\"emb\"]=test_embed[0:2]\n",
    "        graphs_test.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2609\n",
      "2609\n"
     ]
    }
   ],
   "source": [
    "print(len(graphs_test\n",
    "         ))\n",
    "print(len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d752295cf0b346ae994422af874d706c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2609 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "preds=[]\n",
    "with torch.no_grad():\n",
    "    for i,graph in tqdm_notebook(enumerate(graphs_test),total=len(graphs_test)):\n",
    "\n",
    "        node_features=graph.ndata[\"emb\"]\n",
    "        graph=dgl.add_self_loop(graph)\n",
    "        logits = model(graph, node_features)\n",
    "        logits=torch.unsqueeze(logits,0)\n",
    "        pred=logits.detach().cpu().numpy()\n",
    "#         print(pred)\n",
    "        pred=np.argmax(pred,axis=1)\n",
    "        preds.append(pred)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2609\n"
     ]
    }
   ],
   "source": [
    "# graph=graphs_test[1]\n",
    "# node_features=graph.ndata[\"emb\"]\n",
    "# graph=dgl.add_self_loop(graph)\n",
    "# logits = model(graph, node_features)\n",
    "# logits=torch.unsqueeze(logits,0)\n",
    "# pred=logits.detach().cpu().numpy()\n",
    "# print(pred)\n",
    "# pred=np.argmax(pred,axis=1)\n",
    "# print(pred)\n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label=test_label[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2609"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(preds)\n",
    "len(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc= 0.3740896895362208\n",
      "f1 score 0.41275439265281627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"acc=\",accuracy_score(test_label,preds))\n",
    "# print(classification_report(test_label, preds))\n",
    "print(\"f1 score\", f1_score(test_label,preds,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl",
   "language": "python",
   "name": "dgl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
