{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /mnt/berry/home/prakhar/.local/lib/python3.9/site-packages (1.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/berry/home/prakhar/.local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /mnt/berry/home/prakhar/.local/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /mnt/berry/home/prakhar/.local/lib/python3.9/site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/berry/home/linuxbrew/.linuxbrew/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/mnt/berry/home/linuxbrew/.linuxbrew/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "# from matplotlib import style\n",
    "import random\n",
    "\n",
    "# style.use(\"dark_background\")\n",
    "seed_val = 994\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "device=torch.device(\"cuda:5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>also I was the point person on my company’s tr...</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:16,059</td>\n",
       "      <td>00:16:21,731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>You must’ve had your hands full.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:21,940</td>\n",
       "      <td>00:16:23,442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>That I did. That I did.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:23,442</td>\n",
       "      <td>00:16:26,389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So let’s talk a little bit about your duties.</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:26,820</td>\n",
       "      <td>00:16:29,572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>My duties?  All right.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>00:16:34,452</td>\n",
       "      <td>00:16:40,917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance          Speaker  \\\n",
       "0       1  also I was the point person on my company’s tr...         Chandler   \n",
       "1       2                   You must’ve had your hands full.  The Interviewer   \n",
       "2       3                            That I did. That I did.         Chandler   \n",
       "3       4      So let’s talk a little bit about your duties.  The Interviewer   \n",
       "4       5                             My duties?  All right.         Chandler   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0   neutral   neutral            0             0       8       21   \n",
       "1   neutral   neutral            0             1       8       21   \n",
       "2   neutral   neutral            0             2       8       21   \n",
       "3   neutral   neutral            0             3       8       21   \n",
       "4  surprise  positive            0             4       8       21   \n",
       "\n",
       "      StartTime       EndTime  \n",
       "0  00:16:16,059  00:16:21,731  \n",
       "1  00:16:21,940  00:16:23,442  \n",
       "2  00:16:23,442  00:16:26,389  \n",
       "3  00:16:26,820  00:16:29,572  \n",
       "4  00:16:34,452  00:16:40,917  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %history\n",
    "train_df=pd.read_csv(\"./MELD/data/MELD/train_sent_emo.csv\")\n",
    "val_df=pd.read_csv(\"./MELD/data/MELD/dev_sent_emo.csv\")\n",
    "test_df=pd.read_csv(\"./MELD/data/MELD/test_sent_emo.csv\")\n",
    "\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'surprise' 'fear' 'sadness' 'joy' 'disgust' 'anger']\n",
      "9989\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(train_df.Emotion.unique())\n",
    "print(len(train_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4710\n",
      "4    1743\n",
      "1    1205\n",
      "6    1109\n",
      "3     683\n",
      "5     271\n",
      "2     268\n",
      "Name: Emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mapping ={'neutral':0, 'surprise':1, 'fear':2, 'sadness':3, 'joy':4, 'disgust':5, 'anger':6}\n",
    "\n",
    "\n",
    "\n",
    "train_df=train_df.replace({\"Emotion\":mapping})# mapping labels to integer\n",
    "val_df=val_df.replace({\"Emotion\":mapping})\n",
    "test_df=test_df.replace({\"Emotion\":mapping})\n",
    "past_window=1\n",
    "\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "# mapping2={\"positive\":0,\"neutral\":1,\"negative\":2}\n",
    "# train_sent=train_df.replace({\"Sentiment\":mapping2})\n",
    "# train_sent=train_sent[\"Sentiment\"].tolist()\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import f1_score\n",
    "# preds=[]\n",
    "train_text=train_df[\"Utterance\"].tolist()\n",
    "val_text=val_df[\"Utterance\"].tolist()\n",
    "test_text=test_df[\"Utterance\"].tolist()\n",
    "\n",
    "# for i in (train_text):\n",
    "#     sentiment_dict = sid_obj.polarity_scores(i)\n",
    "# #     print(i)\n",
    "#     if sentiment_dict['compound'] >= 0.7 :\n",
    "#         preds.append(0)\n",
    "#     elif sentiment_dict['compound'] <= - 0.7 :\n",
    "#         preds.append(2)\n",
    "#     else :\n",
    "#         preds.append(1)\n",
    "        \n",
    "# print(\"acc=\",accuracy_score(train_sent,preds))\n",
    "# print(classification_report(train_sent, preds))\n",
    "\n",
    "# print(\"f1 score\", f1_score(train_sent,preds,average=\"weighted\"))\n",
    "\n",
    "\n",
    "train_labels=train_df[\"Emotion\"]\n",
    "val_labels=val_df[\"Emotion\"]\n",
    "test_labels=test_df[\"Emotion\"]\n",
    "\n",
    "# print(train_text,train_labels)\n",
    "\n",
    "# len_train=[len(j) for j in train_text]\n",
    "# len_test=[len(i) for i in test_text]\n",
    "\n",
    "# pd.Series(len_train).hist()\n",
    "# pd.Series(len_test).hist()\n",
    "\n",
    "print(train_df.Emotion.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_text[6])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "bert = AutoModel.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train=tokenizer.batch_encode_plus(train_text,\n",
    "                                         max_length=80*past_window,\n",
    "                                         padding=True,\n",
    "                                         truncation=True)\n",
    "# tokens_train=tokenizer(train_text[3],\n",
    "#                                          max_length=80*past_window,\n",
    "#                                          padding=True,\n",
    "#                                          truncation=True)\n",
    "# print(tokens_train)\n",
    "# print(tokenizer.decode(tokens_train[\"input_ids\"]))\n",
    "\n",
    "tokens_val=tokenizer.batch_encode_plus(val_text,\n",
    "                                         max_length=80*past_window,\n",
    "                                         padding=True,\n",
    "                                         truncation=True)\n",
    "\n",
    "tokens_test=tokenizer.batch_encode_plus(test_text,\n",
    "                                         max_length=80*past_window,\n",
    "                                         padding=True,\n",
    "                                         truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq=torch.tensor(tokens_train[\"input_ids\"])\n",
    "train_mask=torch.tensor(tokens_train[\"attention_mask\"])\n",
    "train_label=torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq=torch.tensor(tokens_val[\"input_ids\"])\n",
    "val_mask=torch.tensor(tokens_val[\"attention_mask\"])\n",
    "val_label=torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq=torch.tensor(tokens_test[\"input_ids\"])\n",
    "test_mask=torch.tensor(tokens_test[\"attention_mask\"])\n",
    "test_label=torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset,RandomSampler, SequentialSampler\n",
    "batch_size=8\n",
    "\n",
    "train_data=TensorDataset(train_seq,train_mask,train_label)\n",
    "train_sampler=RandomSampler(train_data)\n",
    "train_dataloader=DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "val_data=TensorDataset(val_seq,val_mask,val_label)\n",
    "val_sampler=SequentialSampler(val_data)\n",
    "val_dataloader=DataLoader(val_data,sampler=val_sampler,batch_size=batch_size)\n",
    "\n",
    "test_data=TensorDataset(test_seq,test_mask,test_label)\n",
    "test_sampler=SequentialSampler(test_data)\n",
    "test_dataloader=DataLoader(test_data,sampler=test_sampler,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "# modules=[bert.encoder.layer[:1]]\n",
    "# for module in modules:\n",
    "#     for param in module.parameters():\n",
    "#         param.requires_grad=False\n",
    "\n",
    "# for name, param in list(bert.named_parameters())[:-79]: \n",
    "#     print('I will be frozen: {}'.format(name)) \n",
    "#     param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_model(nn.Module):\n",
    "    def __init__(self,bert):\n",
    "        super(BERT_model,self).__init__()\n",
    "        \n",
    "        self.bert=bert\n",
    "        \n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "        \n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "#         self.fc1=nn.Linear(768,512)\n",
    "        \n",
    "        self.fc2=nn.Linear(1024,7)\n",
    "        \n",
    "        self.softmax=nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, sent_id ,mask):\n",
    "        \n",
    "        cls_hs=self.bert(sent_id,attention_mask=mask)[1]\n",
    "        \n",
    "#         x=self.fc1(cls_hs)\n",
    "        \n",
    "#         x=self.relu(x)\n",
    "        \n",
    "        x=self.dropout(cls_hs)\n",
    "        \n",
    "        x=self.fc2(x)\n",
    "\n",
    "        \n",
    "        x=self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BERT_model(bert)\n",
    "\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.5.0\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer=AdamW(model.parameters(),\n",
    "                lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# class_weights=compute_class_weight(\"balanced\",np.unique(train_labels),train_labels)\n",
    "# print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights=torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# weights=weights.to(device)\n",
    "\n",
    "# cross_entropy=nn.NLLLoss(weight=weights)  #not using is better\n",
    "cross_entropy=nn.NLLLoss()\n",
    "\n",
    "epochs=5\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "# scheduler=get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0.01*len(train_dataloader),num_training_steps=batch_size*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 233\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    total_loss,total_accuracy=0,0\n",
    "    \n",
    "    total_preds=[]\n",
    "    \n",
    "    for step,batch in tqdm_notebook(enumerate(train_dataloader),total=len(train_dataloader)):\n",
    "        \n",
    "            \n",
    "        batch=[r.to(device) for r in batch]\n",
    "        \n",
    "        sent_id,mask,labels=batch\n",
    "        print(sent_id.requires_grad)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds=model(sent_id,mask)\n",
    "        \n",
    "        \n",
    "        loss=cross_entropy(preds,labels)\n",
    "        \n",
    "        \n",
    "        total_loss=total_loss+loss.item()\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        \n",
    "        \n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "        \n",
    "    avg_loss=total_loss/len(train_dataloader)\n",
    "    \n",
    "    total_preds=np.concatenate(total_preds,axis=0)\n",
    "    \n",
    "    \n",
    "    return avg_loss,total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    print(\"\\n Evaluating...\")\n",
    "     \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss,total_acc=0,0\n",
    "    \n",
    "    total_preds=[]\n",
    "    \n",
    "    for step,batch in tqdm_notebook(enumerate(val_dataloader),total=len(val_dataloader)):        \n",
    "            \n",
    "        batch=[r.to(device) for r in batch]\n",
    "        \n",
    "        sent_id,mask,labels=batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            preds=model(sent_id,mask)\n",
    "            \n",
    "            loss=cross_entropy(preds,labels)\n",
    "            \n",
    "            total_loss=total_loss + loss.item()\n",
    "            \n",
    "            preds=preds.detach().cpu().numpy()\n",
    "            \n",
    "            total_preds.append(preds)\n",
    "            \n",
    "    avg_loss=total_loss/len(val_dataloader)\n",
    "    \n",
    "    total_preds= np.concatenate(total_preds,axis=0)\n",
    "    \n",
    "    return avg_loss,total_preds\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " For epoch1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1233d52c1ae4a9ea2eecaa623218d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3558780/4082766229.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n For epoch{:}/{:}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_preds_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3558780/804387995.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mtotal_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "best_val_loss=float(\"inf\")\n",
    "\n",
    "train_losses=[]\n",
    "val_losses=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\n For epoch{:}/{:}\".format(epoch+1,epochs))\n",
    "    \n",
    "    train_loss,_=train()\n",
    "    \n",
    "    val_loss,val_preds_a=evaluate()\n",
    "\n",
    "  \n",
    "    print(\"shape\",val_preds_a.shape)\n",
    "    val_preds_a=np.argmax(val_preds_a, axis = 1)\n",
    "#     torch.save(model.state_dict(),\"/mnt/elm/prakhar/research/bert_no_context/no_context_roberta{:}.pt\".format(epoch+1))\n",
    "#     if val_loss<best_val_loss:\n",
    "#         best_val_loss=val_loss\n",
    "#         torch.save(model.state_dict(),\"saved_weights-large-lossweight-40tok.pt\")\n",
    "        \n",
    "#     print(\"\\n Training loss=\",train_loss)\n",
    "    print(\"\\n Validation loss=\",val_loss)\n",
    "    print(\"Report for val\")\n",
    "#     print((val_labels))\n",
    "    print(classification_report(val_labels,val_preds_a))\n",
    "    print(\"f1 score\", f1_score(val_labels,val_preds_a,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/mnt/elm/prakhar/research/bert_no_context/no_context_roberta2.pt' #epoch1 best\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d8a55c5f0447bc8f055e39fdf42710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.4861045  -1.2867689  -4.3644743  ... -3.834118   -4.721577\n",
      "  -3.1469903 ]\n",
      " [-0.4963631  -1.9692225  -3.0028994  ... -2.5135484  -4.8845344\n",
      "  -2.6146753 ]\n",
      " [-0.13683453 -3.2623746  -3.7213635  ... -3.787551   -5.381347\n",
      "  -3.7771282 ]\n",
      " ...\n",
      " [-1.9383972  -2.3093572  -4.054474   ... -0.8337363  -4.90138\n",
      "  -1.2849299 ]\n",
      " [-2.5392857  -0.6000536  -2.3237314  ... -3.0497944  -3.226729\n",
      "  -2.1581602 ]\n",
      " [-0.29470897 -4.7901554  -4.447216   ... -1.8653059  -4.292589\n",
      "  -3.2417247 ]]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "total_preds=[]\n",
    "with torch.no_grad():\n",
    "  for step,batch in tqdm_notebook(enumerate(test_dataloader),total=len(test_dataloader)):        \n",
    "            \n",
    "        batch=[r.to(device) for r in batch]\n",
    "        \n",
    "        sent_id,mask,labels=batch\n",
    "                \n",
    "        preds=model(sent_id,mask)\n",
    "\n",
    "\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        total_preds.append(preds)\n",
    "        \n",
    "  total_preds= np.concatenate(total_preds,axis=0)\n",
    "\n",
    "  print(total_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.4861045 -1.2867689 -4.3644743 -3.7899516 -3.834118  -4.721577\n",
      " -3.1469903]\n"
     ]
    }
   ],
   "source": [
    "print(total_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger': 6}\n",
      "acc= 0.6517241379310345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.80      1256\n",
      "           1       0.50      0.62      0.56       281\n",
      "           2       0.29      0.26      0.27        50\n",
      "           3       0.47      0.29      0.36       208\n",
      "           4       0.56      0.65      0.60       402\n",
      "           5       0.50      0.10      0.17        68\n",
      "           6       0.58      0.40      0.47       345\n",
      "\n",
      "    accuracy                           0.65      2610\n",
      "   macro avg       0.52      0.45      0.46      2610\n",
      "weighted avg       0.64      0.65      0.64      2610\n",
      "\n",
      "f1 score 0.6367542302689448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "total_preds = np.argmax(total_preds, axis = 1)\n",
    "print(mapping)\n",
    "print(\"acc=\",accuracy_score(test_labels,total_preds))\n",
    "print(classification_report(test_labels, total_preds))\n",
    "print(\"f1 score\", f1_score(test_labels,total_preds,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight None\n",
      "bert.embeddings.position_embeddings.weight None\n",
      "bert.embeddings.token_type_embeddings.weight None\n",
      "bert.embeddings.LayerNorm.weight None\n",
      "bert.embeddings.LayerNorm.bias None\n",
      "bert.encoder.layer.0.attention.self.query.weight None\n",
      "bert.encoder.layer.0.attention.self.query.bias None\n",
      "bert.encoder.layer.0.attention.self.key.weight None\n",
      "bert.encoder.layer.0.attention.self.key.bias None\n",
      "bert.encoder.layer.0.attention.self.value.weight None\n",
      "bert.encoder.layer.0.attention.self.value.bias None\n",
      "bert.encoder.layer.0.attention.output.dense.weight None\n",
      "bert.encoder.layer.0.attention.output.dense.bias None\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.0.intermediate.dense.weight None\n",
      "bert.encoder.layer.0.intermediate.dense.bias None\n",
      "bert.encoder.layer.0.output.dense.weight None\n",
      "bert.encoder.layer.0.output.dense.bias None\n",
      "bert.encoder.layer.0.output.LayerNorm.weight None\n",
      "bert.encoder.layer.0.output.LayerNorm.bias None\n",
      "bert.encoder.layer.1.attention.self.query.weight None\n",
      "bert.encoder.layer.1.attention.self.query.bias None\n",
      "bert.encoder.layer.1.attention.self.key.weight None\n",
      "bert.encoder.layer.1.attention.self.key.bias None\n",
      "bert.encoder.layer.1.attention.self.value.weight None\n",
      "bert.encoder.layer.1.attention.self.value.bias None\n",
      "bert.encoder.layer.1.attention.output.dense.weight None\n",
      "bert.encoder.layer.1.attention.output.dense.bias None\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.1.intermediate.dense.weight None\n",
      "bert.encoder.layer.1.intermediate.dense.bias None\n",
      "bert.encoder.layer.1.output.dense.weight None\n",
      "bert.encoder.layer.1.output.dense.bias None\n",
      "bert.encoder.layer.1.output.LayerNorm.weight None\n",
      "bert.encoder.layer.1.output.LayerNorm.bias None\n",
      "bert.encoder.layer.2.attention.self.query.weight None\n",
      "bert.encoder.layer.2.attention.self.query.bias None\n",
      "bert.encoder.layer.2.attention.self.key.weight None\n",
      "bert.encoder.layer.2.attention.self.key.bias None\n",
      "bert.encoder.layer.2.attention.self.value.weight None\n",
      "bert.encoder.layer.2.attention.self.value.bias None\n",
      "bert.encoder.layer.2.attention.output.dense.weight None\n",
      "bert.encoder.layer.2.attention.output.dense.bias None\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.2.intermediate.dense.weight None\n",
      "bert.encoder.layer.2.intermediate.dense.bias None\n",
      "bert.encoder.layer.2.output.dense.weight None\n",
      "bert.encoder.layer.2.output.dense.bias None\n",
      "bert.encoder.layer.2.output.LayerNorm.weight None\n",
      "bert.encoder.layer.2.output.LayerNorm.bias None\n",
      "bert.encoder.layer.3.attention.self.query.weight None\n",
      "bert.encoder.layer.3.attention.self.query.bias None\n",
      "bert.encoder.layer.3.attention.self.key.weight None\n",
      "bert.encoder.layer.3.attention.self.key.bias None\n",
      "bert.encoder.layer.3.attention.self.value.weight None\n",
      "bert.encoder.layer.3.attention.self.value.bias None\n",
      "bert.encoder.layer.3.attention.output.dense.weight None\n",
      "bert.encoder.layer.3.attention.output.dense.bias None\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.3.intermediate.dense.weight None\n",
      "bert.encoder.layer.3.intermediate.dense.bias None\n",
      "bert.encoder.layer.3.output.dense.weight None\n",
      "bert.encoder.layer.3.output.dense.bias None\n",
      "bert.encoder.layer.3.output.LayerNorm.weight None\n",
      "bert.encoder.layer.3.output.LayerNorm.bias None\n",
      "bert.encoder.layer.4.attention.self.query.weight None\n",
      "bert.encoder.layer.4.attention.self.query.bias None\n",
      "bert.encoder.layer.4.attention.self.key.weight None\n",
      "bert.encoder.layer.4.attention.self.key.bias None\n",
      "bert.encoder.layer.4.attention.self.value.weight None\n",
      "bert.encoder.layer.4.attention.self.value.bias None\n",
      "bert.encoder.layer.4.attention.output.dense.weight None\n",
      "bert.encoder.layer.4.attention.output.dense.bias None\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.4.intermediate.dense.weight None\n",
      "bert.encoder.layer.4.intermediate.dense.bias None\n",
      "bert.encoder.layer.4.output.dense.weight None\n",
      "bert.encoder.layer.4.output.dense.bias None\n",
      "bert.encoder.layer.4.output.LayerNorm.weight None\n",
      "bert.encoder.layer.4.output.LayerNorm.bias None\n",
      "bert.encoder.layer.5.attention.self.query.weight None\n",
      "bert.encoder.layer.5.attention.self.query.bias None\n",
      "bert.encoder.layer.5.attention.self.key.weight None\n",
      "bert.encoder.layer.5.attention.self.key.bias None\n",
      "bert.encoder.layer.5.attention.self.value.weight None\n",
      "bert.encoder.layer.5.attention.self.value.bias None\n",
      "bert.encoder.layer.5.attention.output.dense.weight None\n",
      "bert.encoder.layer.5.attention.output.dense.bias None\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.5.intermediate.dense.weight None\n",
      "bert.encoder.layer.5.intermediate.dense.bias None\n",
      "bert.encoder.layer.5.output.dense.weight None\n",
      "bert.encoder.layer.5.output.dense.bias None\n",
      "bert.encoder.layer.5.output.LayerNorm.weight None\n",
      "bert.encoder.layer.5.output.LayerNorm.bias None\n",
      "bert.encoder.layer.6.attention.self.query.weight None\n",
      "bert.encoder.layer.6.attention.self.query.bias None\n",
      "bert.encoder.layer.6.attention.self.key.weight None\n",
      "bert.encoder.layer.6.attention.self.key.bias None\n",
      "bert.encoder.layer.6.attention.self.value.weight None\n",
      "bert.encoder.layer.6.attention.self.value.bias None\n",
      "bert.encoder.layer.6.attention.output.dense.weight None\n",
      "bert.encoder.layer.6.attention.output.dense.bias None\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.6.intermediate.dense.weight None\n",
      "bert.encoder.layer.6.intermediate.dense.bias None\n",
      "bert.encoder.layer.6.output.dense.weight None\n",
      "bert.encoder.layer.6.output.dense.bias None\n",
      "bert.encoder.layer.6.output.LayerNorm.weight None\n",
      "bert.encoder.layer.6.output.LayerNorm.bias None\n",
      "bert.encoder.layer.7.attention.self.query.weight None\n",
      "bert.encoder.layer.7.attention.self.query.bias None\n",
      "bert.encoder.layer.7.attention.self.key.weight None\n",
      "bert.encoder.layer.7.attention.self.key.bias None\n",
      "bert.encoder.layer.7.attention.self.value.weight None\n",
      "bert.encoder.layer.7.attention.self.value.bias None\n",
      "bert.encoder.layer.7.attention.output.dense.weight None\n",
      "bert.encoder.layer.7.attention.output.dense.bias None\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.7.intermediate.dense.weight None\n",
      "bert.encoder.layer.7.intermediate.dense.bias None\n",
      "bert.encoder.layer.7.output.dense.weight None\n",
      "bert.encoder.layer.7.output.dense.bias None\n",
      "bert.encoder.layer.7.output.LayerNorm.weight None\n",
      "bert.encoder.layer.7.output.LayerNorm.bias None\n",
      "bert.encoder.layer.8.attention.self.query.weight None\n",
      "bert.encoder.layer.8.attention.self.query.bias None\n",
      "bert.encoder.layer.8.attention.self.key.weight None\n",
      "bert.encoder.layer.8.attention.self.key.bias None\n",
      "bert.encoder.layer.8.attention.self.value.weight None\n",
      "bert.encoder.layer.8.attention.self.value.bias None\n",
      "bert.encoder.layer.8.attention.output.dense.weight None\n",
      "bert.encoder.layer.8.attention.output.dense.bias None\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.8.intermediate.dense.weight None\n",
      "bert.encoder.layer.8.intermediate.dense.bias None\n",
      "bert.encoder.layer.8.output.dense.weight None\n",
      "bert.encoder.layer.8.output.dense.bias None\n",
      "bert.encoder.layer.8.output.LayerNorm.weight None\n",
      "bert.encoder.layer.8.output.LayerNorm.bias None\n",
      "bert.encoder.layer.9.attention.self.query.weight None\n",
      "bert.encoder.layer.9.attention.self.query.bias None\n",
      "bert.encoder.layer.9.attention.self.key.weight None\n",
      "bert.encoder.layer.9.attention.self.key.bias None\n",
      "bert.encoder.layer.9.attention.self.value.weight None\n",
      "bert.encoder.layer.9.attention.self.value.bias None\n",
      "bert.encoder.layer.9.attention.output.dense.weight None\n",
      "bert.encoder.layer.9.attention.output.dense.bias None\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.9.intermediate.dense.weight None\n",
      "bert.encoder.layer.9.intermediate.dense.bias None\n",
      "bert.encoder.layer.9.output.dense.weight None\n",
      "bert.encoder.layer.9.output.dense.bias None\n",
      "bert.encoder.layer.9.output.LayerNorm.weight None\n",
      "bert.encoder.layer.9.output.LayerNorm.bias None\n",
      "bert.encoder.layer.10.attention.self.query.weight None\n",
      "bert.encoder.layer.10.attention.self.query.bias None\n",
      "bert.encoder.layer.10.attention.self.key.weight None\n",
      "bert.encoder.layer.10.attention.self.key.bias None\n",
      "bert.encoder.layer.10.attention.self.value.weight None\n",
      "bert.encoder.layer.10.attention.self.value.bias None\n",
      "bert.encoder.layer.10.attention.output.dense.weight None\n",
      "bert.encoder.layer.10.attention.output.dense.bias None\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.10.intermediate.dense.weight None\n",
      "bert.encoder.layer.10.intermediate.dense.bias None\n",
      "bert.encoder.layer.10.output.dense.weight None\n",
      "bert.encoder.layer.10.output.dense.bias None\n",
      "bert.encoder.layer.10.output.LayerNorm.weight None\n",
      "bert.encoder.layer.10.output.LayerNorm.bias None\n",
      "bert.encoder.layer.11.attention.self.query.weight None\n",
      "bert.encoder.layer.11.attention.self.query.bias None\n",
      "bert.encoder.layer.11.attention.self.key.weight None\n",
      "bert.encoder.layer.11.attention.self.key.bias None\n",
      "bert.encoder.layer.11.attention.self.value.weight None\n",
      "bert.encoder.layer.11.attention.self.value.bias None\n",
      "bert.encoder.layer.11.attention.output.dense.weight None\n",
      "bert.encoder.layer.11.attention.output.dense.bias None\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.11.intermediate.dense.weight None\n",
      "bert.encoder.layer.11.intermediate.dense.bias None\n",
      "bert.encoder.layer.11.output.dense.weight None\n",
      "bert.encoder.layer.11.output.dense.bias None\n",
      "bert.encoder.layer.11.output.LayerNorm.weight None\n",
      "bert.encoder.layer.11.output.LayerNorm.bias None\n",
      "bert.encoder.layer.12.attention.self.query.weight None\n",
      "bert.encoder.layer.12.attention.self.query.bias None\n",
      "bert.encoder.layer.12.attention.self.key.weight None\n",
      "bert.encoder.layer.12.attention.self.key.bias None\n",
      "bert.encoder.layer.12.attention.self.value.weight None\n",
      "bert.encoder.layer.12.attention.self.value.bias None\n",
      "bert.encoder.layer.12.attention.output.dense.weight None\n",
      "bert.encoder.layer.12.attention.output.dense.bias None\n",
      "bert.encoder.layer.12.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.12.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.12.intermediate.dense.weight None\n",
      "bert.encoder.layer.12.intermediate.dense.bias None\n",
      "bert.encoder.layer.12.output.dense.weight None\n",
      "bert.encoder.layer.12.output.dense.bias None\n",
      "bert.encoder.layer.12.output.LayerNorm.weight None\n",
      "bert.encoder.layer.12.output.LayerNorm.bias None\n",
      "bert.encoder.layer.13.attention.self.query.weight None\n",
      "bert.encoder.layer.13.attention.self.query.bias None\n",
      "bert.encoder.layer.13.attention.self.key.weight None\n",
      "bert.encoder.layer.13.attention.self.key.bias None\n",
      "bert.encoder.layer.13.attention.self.value.weight None\n",
      "bert.encoder.layer.13.attention.self.value.bias None\n",
      "bert.encoder.layer.13.attention.output.dense.weight None\n",
      "bert.encoder.layer.13.attention.output.dense.bias None\n",
      "bert.encoder.layer.13.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.13.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.13.intermediate.dense.weight None\n",
      "bert.encoder.layer.13.intermediate.dense.bias None\n",
      "bert.encoder.layer.13.output.dense.weight None\n",
      "bert.encoder.layer.13.output.dense.bias None\n",
      "bert.encoder.layer.13.output.LayerNorm.weight None\n",
      "bert.encoder.layer.13.output.LayerNorm.bias None\n",
      "bert.encoder.layer.14.attention.self.query.weight None\n",
      "bert.encoder.layer.14.attention.self.query.bias None\n",
      "bert.encoder.layer.14.attention.self.key.weight None\n",
      "bert.encoder.layer.14.attention.self.key.bias None\n",
      "bert.encoder.layer.14.attention.self.value.weight None\n",
      "bert.encoder.layer.14.attention.self.value.bias None\n",
      "bert.encoder.layer.14.attention.output.dense.weight None\n",
      "bert.encoder.layer.14.attention.output.dense.bias None\n",
      "bert.encoder.layer.14.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.14.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.14.intermediate.dense.weight None\n",
      "bert.encoder.layer.14.intermediate.dense.bias None\n",
      "bert.encoder.layer.14.output.dense.weight None\n",
      "bert.encoder.layer.14.output.dense.bias None\n",
      "bert.encoder.layer.14.output.LayerNorm.weight None\n",
      "bert.encoder.layer.14.output.LayerNorm.bias None\n",
      "bert.encoder.layer.15.attention.self.query.weight None\n",
      "bert.encoder.layer.15.attention.self.query.bias None\n",
      "bert.encoder.layer.15.attention.self.key.weight None\n",
      "bert.encoder.layer.15.attention.self.key.bias None\n",
      "bert.encoder.layer.15.attention.self.value.weight None\n",
      "bert.encoder.layer.15.attention.self.value.bias None\n",
      "bert.encoder.layer.15.attention.output.dense.weight None\n",
      "bert.encoder.layer.15.attention.output.dense.bias None\n",
      "bert.encoder.layer.15.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.15.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.15.intermediate.dense.weight None\n",
      "bert.encoder.layer.15.intermediate.dense.bias None\n",
      "bert.encoder.layer.15.output.dense.weight None\n",
      "bert.encoder.layer.15.output.dense.bias None\n",
      "bert.encoder.layer.15.output.LayerNorm.weight None\n",
      "bert.encoder.layer.15.output.LayerNorm.bias None\n",
      "bert.encoder.layer.16.attention.self.query.weight None\n",
      "bert.encoder.layer.16.attention.self.query.bias None\n",
      "bert.encoder.layer.16.attention.self.key.weight None\n",
      "bert.encoder.layer.16.attention.self.key.bias None\n",
      "bert.encoder.layer.16.attention.self.value.weight None\n",
      "bert.encoder.layer.16.attention.self.value.bias None\n",
      "bert.encoder.layer.16.attention.output.dense.weight None\n",
      "bert.encoder.layer.16.attention.output.dense.bias None\n",
      "bert.encoder.layer.16.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.16.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.16.intermediate.dense.weight None\n",
      "bert.encoder.layer.16.intermediate.dense.bias None\n",
      "bert.encoder.layer.16.output.dense.weight None\n",
      "bert.encoder.layer.16.output.dense.bias None\n",
      "bert.encoder.layer.16.output.LayerNorm.weight None\n",
      "bert.encoder.layer.16.output.LayerNorm.bias None\n",
      "bert.encoder.layer.17.attention.self.query.weight None\n",
      "bert.encoder.layer.17.attention.self.query.bias None\n",
      "bert.encoder.layer.17.attention.self.key.weight None\n",
      "bert.encoder.layer.17.attention.self.key.bias None\n",
      "bert.encoder.layer.17.attention.self.value.weight None\n",
      "bert.encoder.layer.17.attention.self.value.bias None\n",
      "bert.encoder.layer.17.attention.output.dense.weight None\n",
      "bert.encoder.layer.17.attention.output.dense.bias None\n",
      "bert.encoder.layer.17.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.17.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.17.intermediate.dense.weight None\n",
      "bert.encoder.layer.17.intermediate.dense.bias None\n",
      "bert.encoder.layer.17.output.dense.weight None\n",
      "bert.encoder.layer.17.output.dense.bias None\n",
      "bert.encoder.layer.17.output.LayerNorm.weight None\n",
      "bert.encoder.layer.17.output.LayerNorm.bias None\n",
      "bert.encoder.layer.18.attention.self.query.weight None\n",
      "bert.encoder.layer.18.attention.self.query.bias None\n",
      "bert.encoder.layer.18.attention.self.key.weight None\n",
      "bert.encoder.layer.18.attention.self.key.bias None\n",
      "bert.encoder.layer.18.attention.self.value.weight None\n",
      "bert.encoder.layer.18.attention.self.value.bias None\n",
      "bert.encoder.layer.18.attention.output.dense.weight None\n",
      "bert.encoder.layer.18.attention.output.dense.bias None\n",
      "bert.encoder.layer.18.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.18.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.18.intermediate.dense.weight None\n",
      "bert.encoder.layer.18.intermediate.dense.bias None\n",
      "bert.encoder.layer.18.output.dense.weight None\n",
      "bert.encoder.layer.18.output.dense.bias None\n",
      "bert.encoder.layer.18.output.LayerNorm.weight None\n",
      "bert.encoder.layer.18.output.LayerNorm.bias None\n",
      "bert.encoder.layer.19.attention.self.query.weight None\n",
      "bert.encoder.layer.19.attention.self.query.bias None\n",
      "bert.encoder.layer.19.attention.self.key.weight None\n",
      "bert.encoder.layer.19.attention.self.key.bias None\n",
      "bert.encoder.layer.19.attention.self.value.weight None\n",
      "bert.encoder.layer.19.attention.self.value.bias None\n",
      "bert.encoder.layer.19.attention.output.dense.weight None\n",
      "bert.encoder.layer.19.attention.output.dense.bias None\n",
      "bert.encoder.layer.19.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.19.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.19.intermediate.dense.weight None\n",
      "bert.encoder.layer.19.intermediate.dense.bias None\n",
      "bert.encoder.layer.19.output.dense.weight None\n",
      "bert.encoder.layer.19.output.dense.bias None\n",
      "bert.encoder.layer.19.output.LayerNorm.weight None\n",
      "bert.encoder.layer.19.output.LayerNorm.bias None\n",
      "bert.encoder.layer.20.attention.self.query.weight None\n",
      "bert.encoder.layer.20.attention.self.query.bias None\n",
      "bert.encoder.layer.20.attention.self.key.weight None\n",
      "bert.encoder.layer.20.attention.self.key.bias None\n",
      "bert.encoder.layer.20.attention.self.value.weight None\n",
      "bert.encoder.layer.20.attention.self.value.bias None\n",
      "bert.encoder.layer.20.attention.output.dense.weight None\n",
      "bert.encoder.layer.20.attention.output.dense.bias None\n",
      "bert.encoder.layer.20.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.20.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.20.intermediate.dense.weight None\n",
      "bert.encoder.layer.20.intermediate.dense.bias None\n",
      "bert.encoder.layer.20.output.dense.weight None\n",
      "bert.encoder.layer.20.output.dense.bias None\n",
      "bert.encoder.layer.20.output.LayerNorm.weight None\n",
      "bert.encoder.layer.20.output.LayerNorm.bias None\n",
      "bert.encoder.layer.21.attention.self.query.weight None\n",
      "bert.encoder.layer.21.attention.self.query.bias None\n",
      "bert.encoder.layer.21.attention.self.key.weight None\n",
      "bert.encoder.layer.21.attention.self.key.bias None\n",
      "bert.encoder.layer.21.attention.self.value.weight None\n",
      "bert.encoder.layer.21.attention.self.value.bias None\n",
      "bert.encoder.layer.21.attention.output.dense.weight None\n",
      "bert.encoder.layer.21.attention.output.dense.bias None\n",
      "bert.encoder.layer.21.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.21.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.21.intermediate.dense.weight None\n",
      "bert.encoder.layer.21.intermediate.dense.bias None\n",
      "bert.encoder.layer.21.output.dense.weight None\n",
      "bert.encoder.layer.21.output.dense.bias None\n",
      "bert.encoder.layer.21.output.LayerNorm.weight None\n",
      "bert.encoder.layer.21.output.LayerNorm.bias None\n",
      "bert.encoder.layer.22.attention.self.query.weight None\n",
      "bert.encoder.layer.22.attention.self.query.bias None\n",
      "bert.encoder.layer.22.attention.self.key.weight None\n",
      "bert.encoder.layer.22.attention.self.key.bias None\n",
      "bert.encoder.layer.22.attention.self.value.weight None\n",
      "bert.encoder.layer.22.attention.self.value.bias None\n",
      "bert.encoder.layer.22.attention.output.dense.weight None\n",
      "bert.encoder.layer.22.attention.output.dense.bias None\n",
      "bert.encoder.layer.22.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.22.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.22.intermediate.dense.weight None\n",
      "bert.encoder.layer.22.intermediate.dense.bias None\n",
      "bert.encoder.layer.22.output.dense.weight None\n",
      "bert.encoder.layer.22.output.dense.bias None\n",
      "bert.encoder.layer.22.output.LayerNorm.weight None\n",
      "bert.encoder.layer.22.output.LayerNorm.bias None\n",
      "bert.encoder.layer.23.attention.self.query.weight None\n",
      "bert.encoder.layer.23.attention.self.query.bias None\n",
      "bert.encoder.layer.23.attention.self.key.weight None\n",
      "bert.encoder.layer.23.attention.self.key.bias None\n",
      "bert.encoder.layer.23.attention.self.value.weight None\n",
      "bert.encoder.layer.23.attention.self.value.bias None\n",
      "bert.encoder.layer.23.attention.output.dense.weight None\n",
      "bert.encoder.layer.23.attention.output.dense.bias None\n",
      "bert.encoder.layer.23.attention.output.LayerNorm.weight None\n",
      "bert.encoder.layer.23.attention.output.LayerNorm.bias None\n",
      "bert.encoder.layer.23.intermediate.dense.weight None\n",
      "bert.encoder.layer.23.intermediate.dense.bias None\n",
      "bert.encoder.layer.23.output.dense.weight None\n",
      "bert.encoder.layer.23.output.dense.bias None\n",
      "bert.encoder.layer.23.output.LayerNorm.weight None\n",
      "bert.encoder.layer.23.output.LayerNorm.bias None\n",
      "bert.pooler.dense.weight None\n",
      "bert.pooler.dense.bias None\n",
      "fc2.weight None\n",
      "fc2.bias None\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name,param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl",
   "language": "python",
   "name": "dgl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
